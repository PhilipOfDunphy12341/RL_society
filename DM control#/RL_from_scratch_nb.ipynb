{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib_inline\\config.py:66: DeprecationWarning: InlineBackend._figure_formats_changed is deprecated in traitlets 4.1: use @observe and @unobserve instead.\n",
      "  def _figure_formats_changed(self, name, old, new):\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "from dm_control import mujoco\n",
    "\n",
    "# Access to enums and MuJoCo library functions.\n",
    "from dm_control.mujoco.wrapper.mjbindings import enums\n",
    "from dm_control.mujoco.wrapper.mjbindings import mjlib\n",
    "\n",
    "# PyMJCF\n",
    "from dm_control import mjcf\n",
    "\n",
    "# Composer high level imports\n",
    "from dm_control import composer\n",
    "from dm_control.composer.observation import observable\n",
    "from dm_control.composer import variation\n",
    "\n",
    "# Imports for Composer tutorial example\n",
    "from dm_control.composer.variation import distributions\n",
    "from dm_control.composer.variation import noises\n",
    "from dm_control.locomotion.arenas import floors\n",
    "\n",
    "# Control Suite\n",
    "from dm_control import suite\n",
    "\n",
    "# Run through corridor example\n",
    "from dm_control.locomotion.walkers import cmu_humanoid\n",
    "from dm_control.locomotion.arenas import corridors as corridor_arenas\n",
    "from dm_control.locomotion.tasks import corridors as corridor_tasks\n",
    "\n",
    "# Soccer\n",
    "from dm_control.locomotion import soccer\n",
    "\n",
    "# Manipulation\n",
    "from dm_control import manipulation\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from collections import deque\n",
    "from keras.callbacks import TensorBoard\n",
    "import time\n",
    "import random\n",
    "#HELPER FUNCTION\n",
    "\n",
    "# General\n",
    "import copy\n",
    "import os\n",
    "import itertools\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "# Graphics-related\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 10\n",
    "BIGGER_SIZE = 12\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "# Inline video helper function\n",
    "if os.environ.get('COLAB_NOTEBOOK_TEST', False):\n",
    "  # We skip video generation during tests, as it is quite expensive.\n",
    "  display_video = lambda *args, **kwargs: None\n",
    "else:\n",
    "  def display_video(frames, framerate=30):\n",
    "    height, width, _ = frames[0].shape\n",
    "    dpi = 70\n",
    "    orig_backend = matplotlib.get_backend()\n",
    "    matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
    "    matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
    "    ax.set_axis_off()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_position([0, 0, 1, 1])\n",
    "    im = ax.imshow(frames[0])\n",
    "    def update(frame):\n",
    "      im.set_data(frame)\n",
    "      return [im]\n",
    "    interval = 1000/framerate\n",
    "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                   interval=interval, blit=True, repeat=False)\n",
    "    anim.save('cartpole.mp4', fps=30, extra_args=['-vcodec', 'libx264'])\n",
    "    \n",
    "class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        self.writer = tf.summary.create_file_writer(self.log_dir)\n",
    "        self._log_write_dir = self.log_dir\n",
    "    \n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "        self._train_dir = os.path.join(self._log_write_dir, 'train')\n",
    "        self._train_step = self.model._train_counter\n",
    "    \n",
    "        self._val_dir = os.path.join(self._log_write_dir, 'validation')\n",
    "        self._val_step = self.model._test_counter\n",
    "    \n",
    "        self._should_write_train_graph = False\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "    \n",
    "    def on_train_end(self, _):\n",
    "        pass\n",
    "    \n",
    "    def update_stats(self, **stats):\n",
    "        with self.writer.as_default():\n",
    "            for key, value in stats.items():\n",
    "                tf.summary.scalar(key, value, step = self.step)\n",
    "                self.writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#DQN agent\n",
    "\n",
    "class DQN_agent():\n",
    "    def __init__(self):\n",
    "        self.model = self.create_model()\n",
    "        \n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        self.replay_memory = deque(maxlen=5000)\n",
    "        self.tensorboard = ModifiedTensorBoard(log_dir=\"logs/{}-{}\".format('cartpole', int(time.time())))\n",
    "        self.target_update_counter = 0\n",
    "        \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(5,)))\n",
    "        model.add(Dense(100))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Dense(100))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Dense(3))\n",
    "        model.add(Activation('sigmoid'))\n",
    "        \n",
    "        model.compile(loss = 'mse', optimizer=Adam(lr = 0.001)\n",
    "                      )\n",
    "        return model\n",
    "\n",
    "    def update_replay_memory(self,transition):\n",
    "        self.replay_memory.append(transition)\n",
    "        \n",
    "    def get_qs(self,state):\n",
    "        return self.model.predict(np.array(state))[0]\n",
    "    \n",
    "    def train(self, terminal_state, step):\n",
    "\n",
    "        # Start training only if certain number of samples is already saved\n",
    "        if len(self.replay_memory) < 5000:\n",
    "            return\n",
    "        \n",
    "        # Get a minibatch of random samples from memory replay table\n",
    "        minibatch = random.sample(self.replay_memory, 500)\n",
    "\n",
    "        # Get current states from minibatch, then query NN model for Q values\n",
    "        current_states = np.array([transition[0] for transition in minibatch])\n",
    "        current_qs_list = self.model.predict(current_states)\n",
    "\n",
    "        # Get future states from minibatch, then query NN model for Q values\n",
    "        # When using target network, query it, otherwise main network should be queried\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])\n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        # Now we need to enumerate our batches\n",
    "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "\n",
    "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
    "            # almost like with Q Learning, but we use just part of equation here\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + 0.99 * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            # Update Q value for given state\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "            # And append to our training data\n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "\n",
    "        # Fit on all samples as one batch, log only on terminal state\n",
    "        self.model.fit(np.array(X), np.array(y), batch_size=500, verbose=1, shuffle=False, callbacks=[self.tensorboard] if terminal_state else None)\n",
    "        \n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        # If counter reaches set value, update target network with weights of main network\n",
    "        if self.target_update_counter > 5:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "        self.replay_memory = deque(maxlen=5000)\n",
    "agent = DQN_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EPISODE\n",
    "\n",
    "# Load the environment\n",
    "def run_epispde(epsilon,film,step):\n",
    "  env = suite.load('cartpole', 'balance',visualize_reward=True)\n",
    "\n",
    "  duration = 4  # Seconds\n",
    "  frames = []\n",
    "  ticks = []\n",
    "  rewards = []\n",
    "  observations = []\n",
    "  ep_num = 1\n",
    "  spec = env.action_spec()\n",
    "  time_step = env.reset()\n",
    "  print(f\"Episode {ep_num}\")\n",
    "  while env.physics.data.time < duration:\n",
    "    transition = []\n",
    "    observation = time_step.observation\n",
    "    current_state = np.append(list((observation.items()))[0][1],list((observation.items()))[1][1])\n",
    "    transition.append(current_state)\n",
    "    transition.append(np.argmax(agent.get_qs(current_state.reshape(-1,5))))\n",
    "    if np.random.random()>epsilon:\n",
    "      if np.argmax(agent.get_qs(current_state.reshape(-1,5))) == 0:\n",
    "        action =1\n",
    "      elif np.argmax(agent.get_qs(current_state.reshape(-1,5))) ==1:\n",
    "        action =-1\n",
    "      elif np.argmax(agent.get_qs(current_state.reshape(-1,5)))==2:\n",
    "        action = 0 \n",
    "    else:\n",
    "      action = np.random.uniform(low = -1,high = 1)\n",
    "    time_step = env.step(action)\n",
    "    observation = time_step.observation\n",
    "    new_current_state = np.append(list((observation.items()))[0][1],list((observation.items()))[1][1])\n",
    "    transition.append(time_step.reward)\n",
    "    transition.append(new_current_state)\n",
    "    transition.append(False)\n",
    "    if film == True:\n",
    "      camera0 = env.physics.render(camera_id=0, height=200, width=200)\n",
    "      camera1 = env.physics.render(camera_id=1, height=200, width=200)\n",
    "      frames.append(np.hstack((camera0, camera1)))\n",
    "      observations.append(copy.deepcopy(time_step.observation))\n",
    "      ticks.append(env.physics.data.time)\n",
    "    \n",
    "    agent.update_replay_memory(transition)\n",
    "    done = False\n",
    "    agent.train(done,step)\n",
    "    current_state = new_current_state\n",
    "    step+=1\n",
    "  \n",
    "  else:\n",
    "    done = True\n",
    "    transition[-1]=done\n",
    "    agent.update_replay_memory(transition)\n",
    "    agent.train(done,step)\n",
    "    step+=1\n",
    "\n",
    "\n",
    "  ep_num += 1\n",
    "  \n",
    "  if film == True:\n",
    "    html_video = display_video(frames, framerate=1./env.control_timestep())\n",
    "\n",
    "    # Show video and plot reward and observations\n",
    "    num_sensors = len(time_step.observation)\n",
    "\n",
    "    _, ax = plt.subplots(1 + num_sensors, 1, sharex=True, figsize=(4, 8))\n",
    "    ax[0].plot(ticks, rewards)\n",
    "    ax[0].set_ylabel('reward')\n",
    "    ax[-1].set_xlabel('time')\n",
    "\n",
    "    for i, key in enumerate(time_step.observation):\n",
    "      data = np.asarray([observations[j][key] for j in range(len(observations))])\n",
    "      ax[i+1].plot(ticks, data, label=key)\n",
    "      ax[i+1].set_ylabel(key)\n",
    "\n",
    "    html_video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "Episode 1\n",
      "Episode 1\n",
      "Episode 1\n",
      "Episode 1\n",
      "Episode 1\n",
      "Episode 1\n",
      "Episode 1\n",
      "Episode 1\n"
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "num_episodes = 1000\n",
    "epsilon = 1\n",
    "decay = 0.999\n",
    "min_epsilon = 0.001\n",
    "step = 1\n",
    "for episode in range(1,num_episodes+1):\n",
    "    agent.tensorboard.step = episode\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        run_epispde(epsilon,False,step)\n",
    "    if epsilon*decay > min_epsilon:\n",
    "        epsilon = epsilon*decay\n",
    "run_epispde(epsilon,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n"
     ]
    }
   ],
   "source": [
    "env = suite.load('cartpole', 'balance',visualize_reward=True)\n",
    "film = True\n",
    "duration = 4  # Seconds\n",
    "frames = []\n",
    "ticks = []\n",
    "rewards = []\n",
    "observations = []\n",
    "ep_num = 1\n",
    "spec = env.action_spec()\n",
    "time_step = env.reset()\n",
    "print(f\"Episode {ep_num}\")\n",
    "while env.physics.data.time < duration:\n",
    "  transition = []\n",
    "  observation = time_step.observation\n",
    "  current_state = np.append(list((observation.items()))[0][1],list((observation.items()))[1][1])\n",
    "  transition.append(current_state)\n",
    "  transition.append(np.argmax(agent.get_qs(current_state.reshape(-1,5))))\n",
    "  if np.random.random()>epsilon:\n",
    "    if np.argmax(agent.get_qs(current_state.reshape(-1,5))) == 0:\n",
    "      action =1\n",
    "    elif np.argmax(agent.get_qs(current_state.reshape(-1,5))) ==1:\n",
    "      action =-1\n",
    "    elif np.argmax(agent.get_qs(current_state.reshape(-1,5)))==2:\n",
    "      action = 0 \n",
    "  else:\n",
    "    action = np.random.uniform(low = -1,high = 1)\n",
    "  time_step = env.step(action)\n",
    "  observation = time_step.observation\n",
    "  new_current_state = np.append(list((observation.items()))[0][1],list((observation.items()))[1][1])\n",
    "  transition.append(time_step.reward)\n",
    "  transition.append(new_current_state)\n",
    "  transition.append(False)\n",
    "  if film == True:\n",
    "    camera0 = env.physics.render(camera_id=0, height=200, width=200)\n",
    "    camera1 = env.physics.render(camera_id=1, height=200, width=200)\n",
    "    frames.append(np.hstack((camera0, camera1)))\n",
    "    observations.append(copy.deepcopy(time_step.observation))\n",
    "    ticks.append(env.physics.data.time)\n",
    "  \n",
    "  agent.update_replay_memory(transition)\n",
    "  done = False\n",
    "  #agent.train(done,step)\n",
    "  current_state = new_current_state\n",
    "  step+=1\n",
    "\n",
    "else:\n",
    "  done = True\n",
    "  transition[-1]=done\n",
    "  agent.update_replay_memory(transition)\n",
    "  agent.train(done,step)\n",
    "  step+=1\n",
    "ep_num += 1\n",
    "\n",
    "if film == True:\n",
    "  html_video = display_video(frames, framerate=1./env.control_timestep())\n",
    "  # Show video and plot reward and observations\n",
    "  #num_sensors = len(time_step.observation)\n",
    "  #_, ax = plt.subplots(1 + num_sensors, 1, sharex=True, figsize=(4, 8))\n",
    "  #ax[0].plot(ticks, rewards)\n",
    "  #ax[0].set_ylabel('reward')\n",
    "  #ax[-1].set_xlabel('time')\n",
    "  #for i, key in enumerate(time_step.observation):\n",
    "  #  data = np.asarray([observations[j][key] for j in range(len(observations))])\n",
    "  #  ax[i+1].plot(ticks, data, label=key)\n",
    "  #  ax[i+1].set_ylabel(key)\n",
    "\n",
    "html_video\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6b2678bb939e02708a85109ce432d5ed9cfdd3b231335073a43287bf3ec8a66c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
