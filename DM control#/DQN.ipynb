{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from dm_control import mujoco\n",
    "\n",
    "# Access to enums and MuJoCo library functions.\n",
    "from dm_control.mujoco.wrapper.mjbindings import enums\n",
    "from dm_control.mujoco.wrapper.mjbindings import mjlib\n",
    "\n",
    "# PyMJCF\n",
    "from dm_control import mjcf\n",
    "\n",
    "# Composer high level imports\n",
    "from dm_control import composer\n",
    "from dm_control.composer.observation import observable\n",
    "from dm_control.composer import variation\n",
    "\n",
    "# Imports for Composer tutorial example\n",
    "from dm_control.composer.variation import distributions\n",
    "from dm_control.composer.variation import noises\n",
    "from dm_control.locomotion.arenas import floors\n",
    "\n",
    "# Control Suite\n",
    "from dm_control import suite\n",
    "\n",
    "# Run through corridor example\n",
    "from dm_control.locomotion.walkers import cmu_humanoid\n",
    "from dm_control.locomotion.arenas import corridors as corridor_arenas\n",
    "from dm_control.locomotion.tasks import corridors as corridor_tasks\n",
    "\n",
    "# Soccer\n",
    "from dm_control.locomotion import soccer\n",
    "\n",
    "# Manipulation\n",
    "from dm_control import manipulation\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Input, Dropout, Normalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from collections import deque\n",
    "from keras.callbacks import TensorBoard\n",
    "import time\n",
    "import random\n",
    "#HELPER FUNCTION\n",
    "\n",
    "# General\n",
    "import copy\n",
    "import os\n",
    "import itertools\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "# Graphics-related\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 10\n",
    "BIGGER_SIZE = 12\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "# Inline video helper function\n",
    "if os.environ.get('COLAB_NOTEBOOK_TEST', False):\n",
    "  # We skip video generation during tests, as it is quite expensive.\n",
    "  display_video = lambda *args, **kwargs: None\n",
    "else:\n",
    "  def display_video(frames, framerate=30):\n",
    "    height, width, _ = frames[0].shape\n",
    "    dpi = 70\n",
    "    orig_backend = matplotlib.get_backend()\n",
    "    matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
    "    matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
    "    ax.set_axis_off()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_position([0, 0, 1, 1])\n",
    "    im = ax.imshow(frames[0])\n",
    "    def update(frame):\n",
    "      im.set_data(frame)\n",
    "      return [im]\n",
    "    interval = 1000/framerate\n",
    "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                   interval=interval, blit=True, repeat=False)\n",
    "    anim.save('cartpole.mp4', fps=30, extra_args=['-vcodec', 'libx264'])\n",
    "    \n",
    "class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        self.writer = tf.summary.create_file_writer(self.log_dir)\n",
    "        self._log_write_dir = self.log_dir\n",
    "    \n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "        self._train_dir = os.path.join(self._log_write_dir, 'train')\n",
    "        self._train_step = self.model._train_counter\n",
    "    \n",
    "        self._val_dir = os.path.join(self._log_write_dir, 'validation')\n",
    "        self._val_step = self.model._test_counter\n",
    "    \n",
    "        self._should_write_train_graph = False\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "    \n",
    "    def on_train_end(self, _):\n",
    "        pass\n",
    "    \n",
    "    def update_stats(self, **stats):\n",
    "        with self.writer.as_default():\n",
    "            for key, value in stats.items():\n",
    "                tf.summary.scalar(key, value, step = self.step)\n",
    "                self.writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DQN agent\n",
    "\n",
    "class DQN_agent():\n",
    "    def __init__(self):\n",
    "        self.model = self.create_model()\n",
    "        \n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        self.replay_memory = deque(maxlen=10000)\n",
    "        self.tensorboard = ModifiedTensorBoard(log_dir=\"logs/{}-{}\".format('cartpole', int(time.time())))\n",
    "        self.target_update_counter = 0\n",
    "        \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(4,)))\n",
    "        model.add(Dense(100))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Normalization())\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(100))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Normalization())\n",
    "        model.add(Dense(2))\n",
    "        model.add(Activation('linear'))\n",
    "        \n",
    "        model.compile(loss = 'mse' ,optimizer=Adam(learning_rate=  0.0005))\n",
    "\n",
    "        return model\n",
    "\n",
    "    def update_replay_memory(self,transition):\n",
    "        self.replay_memory.append(transition)\n",
    "        \n",
    "    def get_qs(self,state):\n",
    "        \n",
    "        return self.model.predict(state)[0]\n",
    "    \n",
    "    def train(self, terminal_state, step):\n",
    "\n",
    "        # Start training only if certain number of samples is already saved\n",
    "        if len(self.replay_memory) < 10000:\n",
    "            return\n",
    "        \n",
    "        # Get a minibatch of random samples from memory replay table\n",
    "        minibatch = random.sample(self.replay_memory, 400)\n",
    "\n",
    "        # Get current states from minibatch, then query NN model for Q values\n",
    "        current_states = np.array([transition[0] for transition in minibatch])\n",
    "        current_qs_list = self.model.predict(current_states)\n",
    "\n",
    "        # Get future states from minibatch, then query NN model for Q values\n",
    "        # When using target network, query it, otherwise main network should be queried\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])\n",
    "\n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        # Now we need to enumerate our batches\n",
    "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "\n",
    "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
    "            # almost like with Q Learning, but we use just part of equation here\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + 0.99 * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            # Update Q value for given state\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "            # And append to our training data\n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "\n",
    "        # Fit on all samples as one batch, log only on terminal state\n",
    "        self.model.fit(np.array(X), np.array(y), batch_size=400, verbose=0, shuffle=False, callbacks=[self.tensorboard] if terminal_state else None)\n",
    "        \n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        # If counter reaches set value, update target network with weights of main network\n",
    "        if self.target_update_counter > 5:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load the environment\\ndef run_episode(epsilon,film,step,rewards):\\n  env = suite.load(\\'cartpole\\', \\'balance\\')\\n\\n  duration = 4 # Seconds\\n  frames = []\\n  ticks = []\\n  \\n  observations = []\\n  actions = []\\n  time_step = env.reset()\\n  \\n  while env.physics.data.time < duration:\\n    transition = []\\n    observation = time_step.observation\\n    current_pos = np.array(list((observation.items()))[0][1])\\n    current_vel = np.array(list((observation.items()))[1][1])\\n    current_state = np.concatenate(( current_pos/np.linalg.norm(current_pos),current_vel/np.linalg.norm(current_vel)))\\n    transition.append(current_state)\\n    attempt = agent.get_qs((np.array(current_state)).reshape(-1,5))\\n    attempt_max = np.argmax(attempt)\\n\\n    transition.append(attempt_max)\\n    if np.random.random()>epsilon:\\n      if attempt_max == 0:\\n        action =1\\n      elif attempt_max ==1:\\n        action =-1\\n      elif attempt_max==2:\\n        action = 0 \\n    else:\\n      rand = np.random.random()\\n      if rand >0.8:\\n        action = 1\\n      elif 0.8>rand>0.6:\\n        action =-1\\n      elif 0.6>rand>0.4:\\n        action =0\\n      elif 0.4>rand>0.2:\\n        action =1\\n      else:\\n        action = -1\\n        \\n    actions.append(action)\\n    time_step = env.step(action)\\n    observation = time_step.observation\\n \\n    new_current_pos = np.array(list((observation.items()))[0][1])\\n    new_current_vel = np.array(list((observation.items()))[1][1])\\n    new_current_state = np.concatenate(( new_current_pos/np.linalg.norm(new_current_pos),new_current_vel/np.linalg.norm(new_current_vel)))\\n    \\n    reward =time_step.reward\\n    rewards.append(reward)\\n    transition.append(reward)\\n    transition.append(new_current_state)\\n    done = False\\n    transition.append(done)\\n    agent.update_replay_memory(transition)\\n    \\n    agent.train(done,step)\\n    current_state = new_current_state\\n    if film == True:\\n      camera0 = env.physics.render(camera_id=0, height=200, width=200)\\n      camera1 = env.physics.render(camera_id=1, height=200, width=200)\\n      frames.append(np.hstack((camera0, camera1)))\\n      rewards.append(time_step.reward)\\n      observations.append(copy.deepcopy(time_step.observation))\\n      ticks.append(env.physics.data.time)\\n      \\n    \\n  \\n  \\n  done = True\\n  transition=[current_state,attempt_max,reward,new_current_state,done]\\n  agent.update_replay_memory(transition)\\n  agent.train(done,step)\\n  \\n  \\n  if film == True:\\n    print(\"FILMING\")\\n    html_video = display_video(frames, framerate=1./env.control_timestep())\\n    html_video\\n    return actions\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EPISODE (MuJoCo)\n",
    "'''\n",
    "# Load the environment\n",
    "def run_episode(epsilon,film,step,rewards):\n",
    "  env = suite.load('cartpole', 'balance')\n",
    "\n",
    "  duration = 4 # Seconds\n",
    "  frames = []\n",
    "  ticks = []\n",
    "  \n",
    "  observations = []\n",
    "  actions = []\n",
    "  time_step = env.reset()\n",
    "  \n",
    "  while env.physics.data.time < duration:\n",
    "    transition = []\n",
    "    observation = time_step.observation\n",
    "    current_pos = np.array(list((observation.items()))[0][1])\n",
    "    current_vel = np.array(list((observation.items()))[1][1])\n",
    "    current_state = np.concatenate(( current_pos/np.linalg.norm(current_pos),current_vel/np.linalg.norm(current_vel)))\n",
    "    transition.append(current_state)\n",
    "    attempt = agent.get_qs((np.array(current_state)).reshape(-1,5))\n",
    "    attempt_max = np.argmax(attempt)\n",
    "\n",
    "    transition.append(attempt_max)\n",
    "    if np.random.random()>epsilon:\n",
    "      if attempt_max == 0:\n",
    "        action =1\n",
    "      elif attempt_max ==1:\n",
    "        action =-1\n",
    "      elif attempt_max==2:\n",
    "        action = 0 \n",
    "    else:\n",
    "      rand = np.random.random()\n",
    "      if rand >0.8:\n",
    "        action = 1\n",
    "      elif 0.8>rand>0.6:\n",
    "        action =-1\n",
    "      elif 0.6>rand>0.4:\n",
    "        action =0\n",
    "      elif 0.4>rand>0.2:\n",
    "        action =1\n",
    "      else:\n",
    "        action = -1\n",
    "        \n",
    "    actions.append(action)\n",
    "    time_step = env.step(action)\n",
    "    observation = time_step.observation\n",
    " \n",
    "    new_current_pos = np.array(list((observation.items()))[0][1])\n",
    "    new_current_vel = np.array(list((observation.items()))[1][1])\n",
    "    new_current_state = np.concatenate(( new_current_pos/np.linalg.norm(new_current_pos),new_current_vel/np.linalg.norm(new_current_vel)))\n",
    "    \n",
    "    reward =time_step.reward\n",
    "    rewards.append(reward)\n",
    "    transition.append(reward)\n",
    "    transition.append(new_current_state)\n",
    "    done = False\n",
    "    transition.append(done)\n",
    "    agent.update_replay_memory(transition)\n",
    "    \n",
    "    agent.train(done,step)\n",
    "    current_state = new_current_state\n",
    "    if film == True:\n",
    "      camera0 = env.physics.render(camera_id=0, height=200, width=200)\n",
    "      camera1 = env.physics.render(camera_id=1, height=200, width=200)\n",
    "      frames.append(np.hstack((camera0, camera1)))\n",
    "      rewards.append(time_step.reward)\n",
    "      observations.append(copy.deepcopy(time_step.observation))\n",
    "      ticks.append(env.physics.data.time)\n",
    "      \n",
    "    \n",
    "  \n",
    "  \n",
    "  done = True\n",
    "  transition=[current_state,attempt_max,reward,new_current_state,done]\n",
    "  agent.update_replay_memory(transition)\n",
    "  agent.train(done,step)\n",
    "  \n",
    "  \n",
    "  if film == True:\n",
    "    print(\"FILMING\")\n",
    "    html_video = display_video(frames, framerate=1./env.control_timestep())\n",
    "    html_video\n",
    "    return actions\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Episode openai gym\n",
    "def run_episode_2(epsilon,step,render,rewards):\n",
    "    env = gym.make('CartPole-v1')\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    for i in range(100):\n",
    "        transition = []\n",
    "        if render == True:\n",
    "            env.render()\n",
    "        transition.append(observation)\n",
    "        attempt = agent.get_qs(((observation).reshape(-1,4)))\n",
    "        attempt_max = np.argmax(attempt)\n",
    "        \n",
    "        if np.random.random()>epsilon:\n",
    "            if attempt_max == 0:\n",
    "                action =0\n",
    "            elif attempt_max ==1:\n",
    "                action =1\n",
    "        else:\n",
    "            rand = np.random.random()\n",
    "            if rand >0.5:\n",
    "                action = 0\n",
    "            elif 0.5>rand:\n",
    "                action =1\n",
    "        transition.append(action)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        transition.append(reward)\n",
    "        rewards.append(reward)\n",
    "        transition.append(observation)\n",
    "        transition.append(done)\n",
    "        agent.update_replay_memory(transition)\n",
    "        agent.train(done,step)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQN_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:150: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "0.99\n",
      "Episode 2\n",
      "17\n",
      "0.9801\n",
      "Episode 3\n",
      "81\n",
      "0.9702989999999999\n",
      "Episode 4\n",
      "13\n",
      "0.96059601\n",
      "Episode 5\n",
      "31\n",
      "0.9509900498999999\n",
      "Episode 6\n",
      "12\n",
      "0.9414801494009999\n",
      "Episode 7\n",
      "26\n",
      "0.9320653479069899\n",
      "Episode 8\n",
      "18\n",
      "0.92274469442792\n",
      "Episode 9\n",
      "18\n",
      "0.9135172474836407\n",
      "Episode 10\n",
      "18\n",
      "0.9043820750088043\n",
      "Episode 11\n",
      "21\n",
      "0.8953382542587163\n",
      "Episode 12\n",
      "12\n",
      "0.8863848717161291\n",
      "Episode 13\n",
      "11\n",
      "0.8775210229989678\n",
      "Episode 14\n",
      "25\n",
      "0.8687458127689781\n",
      "Episode 15\n",
      "26\n",
      "0.8600583546412883\n",
      "Episode 16\n",
      "14\n",
      "0.8514577710948754\n",
      "Episode 17\n",
      "28\n",
      "0.8429431933839266\n",
      "Episode 18\n",
      "21\n",
      "0.8345137614500874\n",
      "Episode 19\n",
      "13\n",
      "0.8261686238355865\n",
      "Episode 20\n",
      "18\n",
      "0.8179069375972307\n",
      "Episode 21\n",
      "15\n",
      "0.8097278682212583\n",
      "Episode 22\n",
      "10\n",
      "0.8016305895390458\n",
      "Episode 23\n",
      "11\n",
      "0.7936142836436553\n",
      "Episode 24\n",
      "12\n",
      "0.7856781408072188\n",
      "Episode 25\n",
      "16\n",
      "0.7778213593991465\n",
      "Episode 26\n",
      "13\n",
      "0.7700431458051551\n",
      "Episode 27\n",
      "22\n",
      "0.7623427143471035\n",
      "Episode 28\n",
      "23\n",
      "0.7547192872036325\n",
      "Episode 29\n",
      "17\n",
      "0.7471720943315961\n",
      "Episode 30\n",
      "19\n",
      "0.7397003733882802\n",
      "Episode 31\n",
      "11\n",
      "0.7323033696543974\n",
      "Episode 32\n",
      "19\n",
      "0.7249803359578534\n",
      "Episode 33\n",
      "20\n",
      "0.7177305325982748\n",
      "Episode 34\n",
      "42\n",
      "0.7105532272722921\n",
      "Episode 35\n",
      "27\n",
      "0.7034476949995692\n",
      "Episode 36\n",
      "51\n",
      "0.6964132180495735\n",
      "Episode 37\n",
      "12\n",
      "0.6894490858690777\n",
      "Episode 38\n",
      "9\n",
      "0.682554595010387\n",
      "Episode 39\n",
      "24\n",
      "0.6757290490602831\n",
      "Episode 40\n",
      "16\n",
      "0.6689717585696803\n",
      "Episode 41\n",
      "18\n",
      "0.6622820409839835\n",
      "Episode 42\n",
      "12\n",
      "0.6556592205741436\n",
      "Episode 43\n",
      "14\n",
      "0.6491026283684022\n",
      "Episode 44\n",
      "21\n",
      "0.6426116020847181\n",
      "Episode 45\n",
      "11\n",
      "0.6361854860638709\n",
      "Episode 46\n",
      "16\n",
      "0.6298236312032323\n",
      "Episode 47\n",
      "11\n",
      "0.6235253948912\n",
      "Episode 48\n",
      "14\n",
      "0.617290140942288\n",
      "Episode 49\n",
      "20\n",
      "0.6111172395328651\n",
      "Episode 50\n",
      "10\n",
      "0.6050060671375365\n",
      "Episode 51\n",
      "13\n",
      "0.5989560064661611\n",
      "Episode 52\n",
      "9\n",
      "0.5929664464014994\n",
      "Episode 53\n",
      "10\n",
      "0.5870367819374844\n",
      "Episode 54\n",
      "16\n",
      "0.5811664141181095\n",
      "Episode 55\n",
      "20\n",
      "0.5753547499769285\n",
      "Episode 56\n",
      "13\n",
      "0.5696012024771592\n",
      "Episode 57\n",
      "10\n",
      "0.5639051904523876\n",
      "Episode 58\n",
      "12\n",
      "0.5582661385478638\n",
      "Episode 59\n",
      "14\n",
      "0.5526834771623851\n",
      "Episode 60\n",
      "10\n",
      "0.5471566423907612\n",
      "Episode 61\n",
      "12\n",
      "0.5416850759668536\n",
      "Episode 62\n",
      "11\n",
      "0.536268225207185\n",
      "Episode 63\n",
      "12\n",
      "0.5309055429551132\n",
      "Episode 64\n",
      "15\n",
      "0.525596487525562\n",
      "Episode 65\n",
      "13\n",
      "0.5203405226503064\n",
      "Episode 66\n",
      "15\n",
      "0.5151371174238033\n",
      "Episode 67\n",
      "32\n",
      "0.5099857462495653\n",
      "Episode 68\n",
      "11\n",
      "0.5048858887870696\n",
      "Episode 69\n",
      "10\n",
      "0.4998370298991989\n",
      "Episode 70\n",
      "14\n",
      "0.49483865960020695\n",
      "Episode 71\n",
      "12\n",
      "0.4898902730042049\n",
      "Episode 72\n",
      "35\n",
      "0.48499137027416284\n",
      "Episode 73\n",
      "15\n",
      "0.4801414565714212\n",
      "Episode 74\n",
      "15\n",
      "0.475340042005707\n",
      "Episode 75\n",
      "22\n",
      "0.47058664158564995\n",
      "Episode 76\n",
      "11\n",
      "0.4658807751697934\n",
      "Episode 77\n",
      "11\n",
      "0.4612219674180955\n",
      "Episode 78\n",
      "18\n",
      "0.45660974774391455\n",
      "Episode 79\n",
      "9\n",
      "0.4520436502664754\n",
      "Episode 80\n",
      "13\n",
      "0.44752321376381066\n",
      "Episode 81\n",
      "14\n",
      "0.44304798162617254\n",
      "Episode 82\n",
      "14\n",
      "0.4386175018099108\n",
      "Episode 83\n",
      "14\n",
      "0.4342313267918117\n",
      "Episode 84\n",
      "11\n",
      "0.4298890135238936\n",
      "Episode 85\n",
      "16\n",
      "0.42559012338865465\n",
      "Episode 86\n",
      "13\n",
      "0.4213342221547681\n",
      "Episode 87\n",
      "15\n",
      "0.41712087993322045\n",
      "Episode 88\n",
      "9\n",
      "0.41294967113388825\n",
      "Episode 89\n",
      "8\n",
      "0.40882017442254937\n",
      "Episode 90\n",
      "11\n",
      "0.4047319726783239\n",
      "Episode 91\n",
      "22\n",
      "0.40068465295154065\n",
      "Episode 92\n",
      "12\n",
      "0.39667780642202527\n",
      "Episode 93\n",
      "11\n",
      "0.392711028357805\n",
      "Episode 94\n",
      "12\n",
      "0.38878391807422696\n",
      "Episode 95\n",
      "9\n",
      "0.3848960788934847\n",
      "Episode 96\n",
      "10\n",
      "0.38104711810454983\n",
      "Episode 97\n",
      "14\n",
      "0.37723664692350434\n",
      "Episode 98\n",
      "29\n",
      "0.37346428045426927\n",
      "Episode 99\n",
      "10\n",
      "0.36972963764972655\n",
      "Episode 100\n",
      "11\n",
      "0.36603234127322926\n",
      "Episode 101\n",
      "31\n",
      "0.36237201786049694\n",
      "Episode 102\n",
      "12\n",
      "0.358748297681892\n",
      "Episode 103\n",
      "11\n",
      "0.35516081470507305\n",
      "Episode 104\n",
      "14\n",
      "0.3516092065580223\n",
      "Episode 105\n",
      "20\n",
      "0.34809311449244207\n",
      "Episode 106\n",
      "36\n",
      "0.34461218334751764\n",
      "Episode 107\n",
      "17\n",
      "0.34116606151404244\n",
      "Episode 108\n",
      "33\n",
      "0.337754400898902\n",
      "Episode 109\n",
      "62\n",
      "0.334376856889913\n",
      "Episode 110\n",
      "44\n",
      "0.33103308832101386\n",
      "Episode 111\n",
      "69\n",
      "0.3277227574378037\n",
      "Episode 112\n",
      "69\n",
      "0.3244455298634257\n",
      "Episode 113\n",
      "100\n",
      "0.3212010745647914\n",
      "Episode 114\n",
      "100\n",
      "0.3179890638191435\n",
      "Episode 115\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23836/676764769.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Episode {ep_num}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mrun_episode_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mstep\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m#plt.plot(np.linspace(0,len(rewards),len(rewards)),rewards)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23836/1463399557.py\u001b[0m in \u001b[0;36mrun_episode_2\u001b[1;34m(epsilon, step, render, rewards)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mtransition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_replay_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23836/208711652.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, terminal_state, step)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# Get current states from minibatch, then query NN model for Q values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mcurrent_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtransition\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtransition\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mcurrent_qs_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;31m# Get future states from minibatch, then query NN model for Q values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1876\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Single epoch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1877\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1878\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1879\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1880\u001b[0m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36msteps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1248\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m       \u001b[0moriginal_spe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1251\u001b[0m       can_run_full_execution = (\n\u001b[0;32m   1252\u001b[0m           \u001b[0moriginal_spe\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 674\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    675\u001b[0m     raise NotImplementedError(\n\u001b[0;32m    676\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \"\"\"\n\u001b[0;32m    748\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Read\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 749\u001b[1;33m       \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    750\u001b[0m     \u001b[1;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m     \u001b[1;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m_read_variable_op\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    726\u001b[0m           \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mread_and_set_handle\u001b[1;34m()\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 718\u001b[1;33m       result = gen_resource_variable_ops.read_variable_op(\n\u001b[0m\u001b[0;32m    719\u001b[0m           self.handle, self._dtype)\n\u001b[0;32m    720\u001b[0m       \u001b[0m_maybe_set_handle_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    476\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 478\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m    479\u001b[0m         _ctx, \"ReadVariableOp\", name, resource, \"dtype\", dtype)\n\u001b[0;32m    480\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "\n",
    "num_episodes = 500\n",
    "epsilon = 1\n",
    "decay = 0.99\n",
    "min_epsilon = 0.001\n",
    "step = 1\n",
    "ep_num = 1\n",
    "\n",
    "for episode in range(1,num_episodes+1):\n",
    "    rewards = []\n",
    "    agent.tensorboard.step = episode\n",
    "    done = False\n",
    "    print(f\"Episode {ep_num}\")\n",
    "    run_episode_2(epsilon,step,False,rewards)\n",
    "    step +=1\n",
    "    #plt.plot(np.linspace(0,len(rewards),len(rewards)),rewards)\n",
    "    total = 0\n",
    "    for x in rewards:\n",
    "        if x == 1:\n",
    "            total +=1\n",
    "    print(total)\n",
    "    plt.show()\n",
    "    if epsilon*decay > min_epsilon:\n",
    "        epsilon = epsilon*decay\n",
    "        print(epsilon)\n",
    "    ep_num+=1\n",
    "run_episode_2(epsilon,step,True,rewards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_episode_2(epsilon,step,True,rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6b2678bb939e02708a85109ce432d5ed9cfdd3b231335073a43287bf3ec8a66c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
